{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bf20bd-5a49-4585-8c25-d6d92fb6921a",
   "metadata": {},
   "source": [
    "# Scrape Google reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c7e73-1112-42fa-b5eb-bd0d80b41984",
   "metadata": {},
   "source": [
    "This script scapes Google reviews data from a CSV file of Google Maps links. \n",
    "\n",
    "- It first selects the reviews button and sorts by newest first. It then scrolls to at least the target date (in this case the last 6 months) and calculates relevant metrics.\n",
    "- It scrolls by selecting an xpath on the reviews and hitting the down key untill it is reached. A div number in the xpath is incremented following each successful scroll untill either the target date is met or the same date is detected x number of times and the program moves on.\n",
    "- If an xpath isn't found (likely because the div number is out of range) the program tries again with a closer div number untill it can continue. If Google asks the user to sign in the program presses the back button and re-sorts the reviews by newest first so it can continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e779b61-a223-42b5-97f3-fb0cb7b0c41b",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dbc74-09b0-4446-a490-2f4e413aa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # tabular data \n",
    "import numpy as np # linear algebra\n",
    "from selenium import webdriver # browser automation\n",
    "from webdriver_manager.chrome import ChromeDriverManager # downloads binaries for working with chrome based on your browser\n",
    "from selenium.webdriver.chrome.service import Service # allows chrome driver to be started and stopped \n",
    "from selenium.webdriver.chrome.options import Options # allows for the setting of options\n",
    "from selenium.webdriver.support.ui import WebDriverWait # allows for waiting untill content has loaded\n",
    "from selenium.webdriver.support import expected_conditions as EC # provides conditions that need to be met to know the page has loaded\n",
    "from selenium.webdriver.common.by import By # allows you to find and interact with different elements on a page\n",
    "from selenium.webdriver.common.keys import Keys # allows you to send simulated key strokes\n",
    "from selenium.webdriver.common.action_chains import ActionChains # a mechanism to build sequences of user actions and execute them in a single go\n",
    "import time # allows for pauses to sumulate human behavior\n",
    "from bs4 import BeautifulSoup # parses text from html content\n",
    "import re # strings and regex\n",
    "from datetime import datetime # for working with dates\n",
    "\n",
    "options = Options() # creates an options object\n",
    "options.add_experimental_option(\"detach\", True) # sets chrome to be launched in a new window\n",
    "\n",
    "links = pd.read_csv('links.csv', encoding='utf-8')\n",
    "links = links.loc[162:162,:]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714296df-dd56-4b38-b3f0-feda8aa6ce23",
   "metadata": {},
   "source": [
    "## Key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5817450-6ebd-4215-a803-a0dc6b6a8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number the total reviews are divided by to get a scroll sprint estimate\n",
    "scroll_divider = 15\n",
    "\n",
    "# the starting div number the in the xpath the simulated user selects to scroll to\n",
    "starting_div_num = 6\n",
    "\n",
    "# the amount by which the div number in the xpath is increased every successful scroll\n",
    "div_num_incrementor = 6\n",
    "\n",
    "# the number of scroll sprints - a larger number is beneficial in case there are many reviews\n",
    "retry_num = 50\n",
    "\n",
    "# the number of scroll sprints to go through before the program assumes there is nothing left to scroll and breaks\n",
    "final_same_date_instances = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11df7b-3246-4654-9c67-f67a6e20da25",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cc8c2-fb2e-4430-95fa-e38b6d163730",
   "metadata": {},
   "source": [
    "### Prepare Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36484750-ded4-4d6e-8223-701aaab1475e",
   "metadata": {},
   "source": [
    "Opens a Google maps link in Chrome, selects reviews and sorts by newest first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a13fc-3128-4443-ad1a-572c6aa4a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver(url): # creates a driver (opens chrome)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=options)\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "def click_reviews(driver): # navigates to reviews\n",
    "    xpath = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]'\n",
    "    results = WebDriverWait(driver, 2).until(\n",
    "        EC.presence_of_element_located((By.XPATH,xpath))\n",
    "    )\n",
    "    results.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "def click_sort(driver): # pulls up the sort options\n",
    "    try:\n",
    "        xpath = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[7]/div[2]/button/span/span'\n",
    "        results = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_element_located((By.XPATH,xpath))\n",
    "        )\n",
    "        results.click()\n",
    "    except:\n",
    "        xpath = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[5]/div[2]/button/span/span'\n",
    "        results = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_element_located((By.XPATH,xpath))\n",
    "        )\n",
    "        results.click() \n",
    "\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "def select_newest_reviews(driver): # selects sort by newest\n",
    "    xpath = '//*[@id=\"action-menu\"]/div[2]'\n",
    "    results = WebDriverWait(driver, 2).until(\n",
    "        EC.presence_of_element_located((By.XPATH,xpath))\n",
    "    )\n",
    "    results.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "def sort_by_newest_reviews(driver): # naviagtes to reviews and sorts by newest\n",
    "    click_reviews(driver)\n",
    "    click_sort(driver)\n",
    "    select_newest_reviews(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7874e86-226a-4014-a060-27a8b8241af1",
   "metadata": {},
   "source": [
    "### Estimate scrolls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d4e03-44cd-4d03-82fa-800d799dd023",
   "metadata": {},
   "source": [
    "Estimates the optimum number scrolls as a series of 'sprints' to efficiently scroll to the target date based on overall review count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd123d56-a883-4d4a-8ee3-3c2c6e7fba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html(soup, tag, _class): # extracts all html elements\n",
    "    elements = soup.find_all(tag, class_=_class)\n",
    "    return [x.text for x in elements]\n",
    "\n",
    "def get_overall_review_count(driver): # extracts review count by locating its class\n",
    "    soup = get_soup(driver)\n",
    "    fontBodySmall_class = extract_html(soup, 'div', 'fontBodySmall')\n",
    "    for item in fontBodySmall_class:\n",
    "        if re.search('reviews', item):\n",
    "            try:\n",
    "                return int(item.split()[0].replace(',', ''))\n",
    "            except:\n",
    "                return int(item[0])\n",
    "    print('no data found')\n",
    "    return None\n",
    "\n",
    "def get_scroll_count(driver): # calculates optimum scroll sprints with a lower limit of 10\n",
    "    overall_review_count = get_overall_review_count(driver) \n",
    "    scrolls = round(overall_review_count / scroll_divider)\n",
    "    if scrolls < 10:\n",
    "        print('scrolls: ' + '10')\n",
    "        return 10\n",
    "    else:\n",
    "        print('scrolls: ' + str(scrolls))\n",
    "        return scrolls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11f501-4236-4a8f-a251-c457d650b3d4",
   "metadata": {},
   "source": [
    "### Scroll down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4277d-22d3-45e4-9703-7315b5453bc4",
   "metadata": {},
   "source": [
    "Scrolls down after page load and opens all 'More' links to get the entire review and response texts. Div numbers in xpaths are incremented by 6 at a time. New elements are located, clicked on and scrolled to. If a div number isn't found, it is reduced by 1 next time to ensure it doesn't accidentally become out of range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d8327-b779-4b28-bf92-fd38954e8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_div(driver, sub_div_num, div_num): # waits until it finds a new xpath\n",
    "    xpath = f'//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[{sub_div_num}]/div[{div_num}]'\n",
    "    div = WebDriverWait(driver, 3).until(\n",
    "        EC.presence_of_element_located((By.XPATH,xpath))\n",
    "    )  \n",
    "    return div\n",
    "\n",
    "def hit_down_until_div_reached(driver, div): # clicks on and scrolls to an element\n",
    "    action = ActionChains(driver)\n",
    "    action.move_to_element(div).click().perform() \n",
    "    action.send_keys(Keys.PAGE_DOWN).perform()\n",
    "\n",
    "def press_down_key(driver, sub_div_num, div_num): # increments or substracts div number if found or not\n",
    "    try:\n",
    "        div = find_div(driver, sub_div_num, div_num) \n",
    "        hit_down_until_div_reached(driver, div)\n",
    "        return div_num + div_num_incrementor\n",
    "    except:\n",
    "        print('failed to scroll' + ' - div num: ' + str(div_num))\n",
    "        return div_num - 1\n",
    "\n",
    "def click_more_buttons(driver): # clicks on and opens all 'More' buttons\n",
    "    try:\n",
    "        parent_container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '#QA0Szd > div > div > div.w6VYqd > div:nth-child(2) > div > div.e07Vkf.kA9KIf > div > div > div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde'))\n",
    "        )\n",
    "        more_buttons = parent_container.find_elements(\n",
    "            By.XPATH, \".//button[contains(., 'More')]\"\n",
    "        )\n",
    "        button_count = 0\n",
    "        for button in more_buttons:\n",
    "            if button.text.strip() == 'More':\n",
    "                button.click()\n",
    "                button_count += 1\n",
    "        if button_count > 0:\n",
    "            #print('More button(s) clicked: ' + str(button_count))\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def scroll_down(div_num, sub_div_num, driver, scrolls): # excecutes a scroll sprint and returns the current div number\n",
    "    for scroll in range(scrolls):\n",
    "        div_num = press_down_key(driver, sub_div_num, div_num)\n",
    "        click_more_buttons(driver)\n",
    "        time.sleep(0.5)\n",
    "    return div_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66480e1-1a7c-40d7-b364-798ad8458dfa",
   "metadata": {},
   "source": [
    "### Check dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cb4a7-da51-4019-b192-cc7d9e8763f4",
   "metadata": {},
   "source": [
    "At the end of each scroll sprint, the target date is checked by testing whether the target date is met. If the target date is not met, the current and previous dates are examined to see whether they are the same. If so, we have probably reached the end of the reviews (or a prolonged server error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3675585-24fd-4a41-9aff-2bc302ea5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(driver): # returns html content\n",
    "    page_content = driver.page_source\n",
    "    return BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "def get_dates(driver): # extracts date text by their class\n",
    "    soup = get_soup(driver) \n",
    "    return extract_html(soup, 'span', 'rsqaWe')\n",
    "\n",
    "def get_date_quantifier(driver): # returns the quantifier for the latest date\n",
    "    dates = get_dates(driver)\n",
    "    current_date = dates[len(dates)-1]\n",
    "    return current_date.split()[1]\n",
    "    \n",
    "def target_date_met(driver, div_num): # checks if the target date of two quarters has been met\n",
    "    dates = get_dates(driver)\n",
    "    current_date = dates[len(dates)-1]\n",
    "    for i in range(6,12):\n",
    "        test_month = str(i) + ' months ago'\n",
    "        if (current_date == test_month) or ('year' in current_date.split()) or ('years' in current_date.split()):\n",
    "            print()\n",
    "            print('target date met: finished scrolling')\n",
    "            return True\n",
    "        else:\n",
    "            pass\n",
    "    print('target date not met - current date ' + current_date)\n",
    "\n",
    "def same_date_detected(driver, previous_date): # checks if the new date is the same as the previous\n",
    "    dates = get_dates(driver)\n",
    "    current_date = dates[len(dates)-1]\n",
    "    if current_date ==  previous_date:\n",
    "        return True, ''\n",
    "    else:\n",
    "        return False, current_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa44052-1dfc-4a3a-8a13-9137809bfbca",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c01a3-4098-4128-a974-b9a90b462dcb",
   "metadata": {},
   "source": [
    "Loops through up to 50 scroll sprints to get at least a years worth of reviews. Breaks at the end of each sprint if a year is reached or the same date is detected. If the same date is detected, a False value for the quarter_achieved variable is returned, indicating quarterly percentage change on year metrics need not be calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16908562-fa71-4078-872d-95ea8321ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(driver, sub_div_num): # extracts at least a years worth of reviews if available\n",
    "    scrolls = get_scroll_count(driver)\n",
    "    div_num, previous_date, retries, same_date_instances = starting_div_num, 'today', retry_num, 0\n",
    "    \n",
    "    for i in range(retries):\n",
    "        div_num = scroll_down(div_num, sub_div_num, driver, scrolls)\n",
    "        try:\n",
    "            if target_date_met(driver, div_num):\n",
    "                return driver, driver.page_source, get_soup(driver), get_dates(driver), True\n",
    "                \n",
    "            else:\n",
    "                same_date_as_previous, previous_date = same_date_detected(driver, previous_date)\n",
    "                \n",
    "                if same_date_as_previous:\n",
    "                    same_date_instances += 1\n",
    "\n",
    "                    if same_date_instances == final_same_date_instances:\n",
    "                        print('Same date detected: breaking')\n",
    "                        return driver, driver.page_source, get_soup(driver), get_dates(driver), False\n",
    "        except:\n",
    "            driver.back()\n",
    "            click_sort(driver)\n",
    "            select_newest_reviews(driver)\n",
    "            div_num = 30\n",
    "            #driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "\"\"\" DEBUGGING CODE\n",
    "links_df = pd.read_csv('test_links.csv', encoding='utf-8')\n",
    "name = links_df.name[0]\n",
    "url = links_df.link[0]\n",
    "sub_div_num = links_df.sub_div[0]\n",
    "\n",
    "print('working on: ' + name)\n",
    "driver = create_driver(url)\n",
    "time.sleep(1)\n",
    "sort_by_newest_reviews(driver)\n",
    "driver, page_content, soup, dates, quarter_achieved = extract_data(driver, sub_div_num)\n",
    "time.sleep(1)\n",
    "\n",
    "dates[len(dates)-1]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc7227-f641-4b8b-8851-95fe0ffa8840",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a8110-3e0b-4800-9986-c43f6e2517bf",
   "metadata": {},
   "source": [
    "### Split html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210fad6-a2e0-4dbc-a6bb-799ba4f622c1",
   "metadata": {},
   "source": [
    "The html content is first split into separate yearly and quarterly (latest 3 months) variables. If there are no reviews older than these time periods, everything is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03967fb-4355-461f-a16c-f01b3ea3b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_soup(page_content, soup): # limits the html page content into the latest 6 months\n",
    "    for num in [x for x in range(7, 12)]:\n",
    "        try:\n",
    "            target_soup = page_content.split(f'rsqaWe\">{num} months')[0]\n",
    "            if target_soup != page_content:\n",
    "                return BeautifulSoup(target_soup, 'html.parser')\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    target_soup = page_content.split('rsqaWe\">a year')[0]\n",
    "    if target_soup != page_content:\n",
    "        return BeautifulSoup(target_soup, 'html.parser')\n",
    "    else:\n",
    "        for num in [x for x in range(2, 12)]:\n",
    "            target_soup = page_content.split(f'rsqaWe\">{num} years')[0]\n",
    "            if target_soup != page_content:\n",
    "                return BeautifulSoup(target_soup, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_quarterly_soup(page_content, soup): # limits the html page content into the latest 3 months\n",
    "    for num in [x for x in range(4, 12)]:\n",
    "        try:\n",
    "            first_3_months = page_content.split(f'rsqaWe\">{num} months')[0]\n",
    "            if first_3_months != page_content:\n",
    "                return BeautifulSoup(first_3_months, 'html.parser')\n",
    "        except:\n",
    "            pass\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630ae81-8407-45eb-9ef3-98bd780827d2",
   "metadata": {},
   "source": [
    "### Calculate initial metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219dcc5-be0c-40d9-b430-e0fa50162b54",
   "metadata": {},
   "source": [
    "Calculates the initial metrics asked for by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b45330-83db-4898-bddf-67e215cb1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_reviews(target_soup): # returns target review count\n",
    "    try:\n",
    "        return len(extract_html(target_soup, 'span', 'rsqaWe'))\n",
    "    except:\n",
    "        print('Unable to calculate target reviews')\n",
    "        return ''\n",
    "\n",
    "def get_quarterly_reviews(quarterly_soup): # return quarterly review count\n",
    "    try:\n",
    "        return len(extract_html(quarterly_soup, 'span', 'rsqaWe'))\n",
    "    except:\n",
    "        print('Unable to calculate quarterly reviews')\n",
    "        return ''\n",
    "\n",
    "def get_average_stars(soup): # returns average stars\n",
    "    try:\n",
    "        results = str(soup).split('<div class=\"DU9Pgb\"><span aria-label=\"')\n",
    "        stars = []\n",
    "        for i, result in enumerate(results):\n",
    "            if i != 0 and i != (len(results)-1):\n",
    "                stars.append(int(result[0]))\n",
    "        return round(np.mean(stars), 2)\n",
    "    except:\n",
    "        print('Unable to calculate average stars')\n",
    "        return ''\n",
    "\n",
    "def get_replies(soup): # returns relies for yearly or quarterly soup \n",
    "    try:\n",
    "        return len(extract_html(soup, 'div', 'wiI7pd'))\n",
    "    except:\n",
    "        print('Unable to calculate replies')\n",
    "        return ''\n",
    "\n",
    "def get_reply_rate(soup, reviews_count): # returns reply rate for yearly or quarterly soup \n",
    "    try:\n",
    "        return round((get_replies(soup) / reviews_count) * 100)\n",
    "    except:\n",
    "        print('Unable to calculate reply rate')\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def get_initial_metrics(target_soup, quarterly_soup, soup): # combines the initial metrics\n",
    "    target_reviews, quarterly_reviews = get_target_reviews(target_soup), get_quarterly_reviews(quarterly_soup)\n",
    "    average_stars, target_replies = get_average_stars(target_soup), get_replies(target_soup)\n",
    "    target_reply_rate = get_reply_rate(target_soup, target_reviews)\n",
    "    return target_reviews, quarterly_reviews, average_stars, target_replies, target_reply_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb45df-f317-4c98-b2f9-34f94d8018d8",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0943e39-1b23-4a44-bd79-09d819e3da01",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6957b8-27f3-44ea-87c2-9b0bab6bcda2",
   "metadata": {},
   "source": [
    "Calculates and prints out metrics before storing them in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bf21d-ca1d-4c4f-9e91-378219e6c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(page_content, soup, quarter_achieved): # processes the calculations an creates a pandas dataframe\n",
    "    target_soup = get_target_soup(page_content, soup)\n",
    "    quarterly_soup = get_quarterly_soup(page_content, soup)\n",
    "    target_reviews, quarterly_reviews, average_stars, target_replies, target_reply_rate = get_initial_metrics(target_soup, quarterly_soup, soup)\n",
    "    initial_metrics = ['target_reviews', 'quarterly_reviews', 'average_stars', 'target_replies', 'target_reply_rate']              \n",
    "    \n",
    "    print()\n",
    "    print('Initial metrics: ')\n",
    "    print()\n",
    "    for metric in initial_metrics:\n",
    "        print(metric + ': ' + str(eval(metric)))\n",
    "    print()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df.loc[0, 'dealer'] = name\n",
    "    df.loc[0, 'scrape_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    df.loc[0, 'complete_data'] = str(quarter_achieved)\n",
    "    \n",
    "    for metric in initial_metrics:\n",
    "        df.loc[0, metric] = eval(metric)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\"\"\" DEBUGGING CODE\n",
    "links_df = links.reset_index(drop=True)\n",
    "name = links_df.name[0]\n",
    "url = links_df.link[0]\n",
    "sub_div_num = links_df.sub_div[0]\n",
    "\n",
    "print('working on: ' + name)\n",
    "driver = create_driver(url)\n",
    "time.sleep(1)\n",
    "sort_by_newest_reviews(driver)\n",
    "time.sleep(1)\n",
    "driver, page_content, soup, dates, quarter_achieved = extract_data(driver, sub_div_num)\n",
    "\n",
    "if not quarter_achieved:\n",
    "    print()\n",
    "    print('quarter not achieved')\n",
    "    print()\n",
    "\n",
    "target_soup = get_target_soup(page_content, soup)\n",
    "quarterly_soup = get_quarterly_soup(page_content, soup)\n",
    "\n",
    "target_reviews, quarterly_reviews, average_stars, target_replies, target_reply_rate = get_initial_metrics(target_soup, quarterly_soup, soup)\n",
    "\n",
    "df = create_dataframe(page_content, soup, quarter_achieved)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264b67c-368e-4064-a330-c7f61653de90",
   "metadata": {},
   "source": [
    "## Save dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fe970-c1b3-4da8-98d9-ccb5ec5d7dc7",
   "metadata": {},
   "source": [
    "Each new row containing the metrics and review/reply texts is joined with the existing dataframe and saved before moving on to the next one. This prevents data loss should an error occur at some point during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f462e-326b-46d9-8257-12a5af71c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_frame(df): # concatenates each new row with the existing dataframe and saves it\n",
    "    old_df = pd.read_csv('review_data.csv', encoding='utf-8')\n",
    "    new_df = pd.concat([old_df, df], axis=0).reset_index(drop=True)\n",
    "    new_df.to_csv('review_data.csv', index=False)\n",
    "    print()\n",
    "    print('data saved')\n",
    "    print()\n",
    "    print('---')\n",
    "\n",
    "for i in list(links.index): # final loop - opens chrome, scrolls, calculates metrics, saves the data and closes chrome\n",
    "    name = links.name[i]\n",
    "    url = links.link[i]\n",
    "    sub_div_num = links.sub_div[i]\n",
    "    print()\n",
    "    print('working on: ' + name)\n",
    "    print()\n",
    "    driver = create_driver(url)\n",
    "    time.sleep(1)\n",
    "    sort_by_newest_reviews(driver)\n",
    "    time.sleep(1)\n",
    "    driver, page_content, soup, dates, quarter_achieved = extract_data(driver, sub_div_num)\n",
    "    print()\n",
    "    df = create_dataframe(page_content, soup, quarter_achieved)\n",
    "    save_data_frame(df)\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
